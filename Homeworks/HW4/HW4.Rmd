---
title: "Homework 4"
author: "Sneha Karanjai"
date: "9/18/2022"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: readable
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction 
The goal of this homework is to get practice reading in raw data from different sources. Some files are
available in the homework link, others via a URL, and others may be connected to using a package of some
type. 

# Part 1 

## 1. If your working directory is `myfolder/homework/`, what path would you specify to get the file located
at `myfolder/MyData.csv`?

To get the data MyData.csv, the path we specify is `../MyData.csv`. The '../' takes us one directory behind.

## 2. What are the major benefits of using R projects? Should you be using an R project for each homework assignment (or at least for the course)??

Projects help us in creating reproducible code. Packages help us in collaboration through Version Control in Github. The way packages help in collaboration is that if we build individual R programs and then share it across to other people then our working directory will not match theirs. Hence, they would have to change the paths for individual file imports. Projects help in making sure that this problem does not exist. To summarise, Projects help in : 
- Collaborating 
- Reproducibility 
- Easier Imports 

Instead of creating a new project for each homework, it is better to have a single project for the entire course. 

## 3. What is git and what is github?
Git is an open-source, version control tool created in 2005 by developers working on the Linux operating system. GitHub is a company founded in 2008 that makes tools which integrate with git. You do not need GitHub to use git, but you cannot use GitHub without using git.

# Part 2 
The purpose of this part is to read in delimited dataset in R. The source of the datasets is UCI Machine Learning Repository. 

## Importing necessary libraries 

```{r}
library(readr)
library(dplyr)
```


## Glass Data 

### 1. Read this data into R using functions from the tidyverse. Notice that the data doesn’t include column
names - add those (in a manner of your choosing). Print out the tibble (just call the object name).

This dataset is a comma delimited or a .csv file. This means that the value of each column is separated by a comma. To read in the data we will use the `read_csv()` function from the tidyverse's `readr` package. Reading the data into R environment involves adding the column names manually as the dataframe in its original format does not include the header row. Print it out as a `tibble`. 

```{r}
glass_data <- read_csv("https://www4.stat.ncsu.edu/~online/datasets/glass.data",
                       col_names = c("Id", "RI", "Na", "Mg", "Al", "Si", "K", "Ca", "Ba", "Fe", "Type_Of_Glass"))
glass_data
```
### 2. Overwrite the `Type_of_glass` variable by creating a factor there instead. Use the variable descriptions above to give meaningful factor levels.

We see that the `Type_Of_Glass` column is read in as a double because of the values of the columns being from 1-7. We need to overwrite this to a factor variable. Factor in R is a variable used to categorize and store the data, having a limited number of different values. We use the `factor()` function from the base package. 

```{r}
glass_data$Type_Of_Glass <- factor(glass_data$Type_Of_Glass) 
glass_data
```

Now that we have converted the data type of `Type_Of_Glass`, it is time for us re-label the factors. The labels need to be factored : 

- 1 building_windows_float_processed
- 2 building_windows_non_float_processed
- 3 vehicle_windows_float_processed
- 4 vehicle_windows_non_float_processed (none in this database)
- 5 containers
- 6 tableware
- 7 headlamps

For this we use the `recode()` function from the `dplyr` package. 

```{r}
glass_data$Type_Of_Glass <- recode(glass_data$Type_Of_Glass, 
                                          "1" = "building_windows_float_processed",
                                          "2" = "building_windows_non_float_processed",
                                          "3" = "vehicle_windows_float_processed", 
                                          "4" = "vehicle_windows_non_float_processed", 
                                          "5" = "containers", 
                                          "6" = "tableware",
                                          "7" = "headlamps")
glass_data
```

### 3. Print the data frame with only observations where the `Fe` variable is less than 0.2 and the `Type of Glass `is either tableware or headlamp.
Success! Now we filter the dataset using the `filter()` function to display records with `Fe` value less than 0.2 and `Type_Of_Glass` value either tableware or headlamps.

```{r}
glass_data %>% 
  filter((Fe < 0.2) & Type_Of_Glass %in% c("tableware", "headlamps"))
```

## Yeast Data 

### 1. Read this data into R using functions from the tidyverse. Notice that the data doesn’t include column names - add those (in a manner of your choosing). Print out the tibble (just call the object name).

The first task is the read the data from the URL provided. We notice that the raw form of the dataset does not include column names so we manually put that in while reading the dataset based on the information provided to us. 

```{r}
yeast_data <- read_table("https://www4.stat.ncsu.edu/~online/datasets/yeast.data", 
                       col_names = c("seq_name", "mcg", "gvh", "alm", "mit", "erl", "pox", "vac", "nuc", "class"))
yeast_data
```

### 2. Select only the class and mcg columns. Report the mean and standard deviation of the mcg value for each setting of the class variable

Now that we have the data in, it is time for data manipulation activities. We select class and mcg, we then group_by class and finally summarise the dataset to get the mean and standard deviation of mcg. 

```{r}
yeast_data %>% 
  select(class, mcg) %>% 
  group_by(class) %>% 
  summarise(mean_mcg = mean(mcg), std_mcg = sd(mcg))
```

# Part 3 - Database 

## Chinook DB
We will be working with an example SQLite database called chinook.

### 1. Download the chinook.db database. (If needed install and) load the DBI and RSQLite packages, and load the tidyverse package. Use dbConnect() to connect to the this local database.

First step is to download the chinook.db from Moodle. We then install and load `DBI` and `RSQLite` packages, alongwith the `tidyverse` package. Further, we use `dbConnect()` to connect to the local DB to reach out to the chinook.db. 

```{r}
#install.packages("DBI")
#install.packages("RSQLite")
library("DBI")
library("RSQLite")
library("dbplyr")

con <- dbConnect(RSQLite::SQLite(), "chinook.db")
dbListTables(conn = con)
```

### 2. Now print out the tables in the database using `dbListTables()`.

The `dbListTables()` function lists out all the tables in the given database. 

```{r}
dbListTables(conn = con)
```

### 3. Use `dbGetQuery()` or `tbl()` to grab and print out the invoices table and the customers table.

We use `tbl()` which is a generic method that dispatches based on the first argument. We grab the invoices and customers table. 

```{r}
invoices <- tbl(con, "invoices")
invoices
customers <- tbl(con, "customers")
customers
```

### 4. Use an inner_join() to combine the two tables above by the CustomerID variable.

`inner_join()` is the mutating joins add columns from y to x, matching rows based on the keys that includes all rows in both x and y.

```{r}
invoices %>% 
  inner_join(customers, by = "CustomerId")
```

# Part 4. Querying an API 

## NEWS API

### 1. Use `GET` from the `httr` package to return information about a topic that you are interested in that has been in the news lately. Select only the source, author, and title columns and print the tibble out.

```{r}
#install.packages("httr")
library("httr")
library(jsonlite)
```

`GET` is a function that does it what it says. It GETs a url. It is from the `httr` package. The URL we use is from the https://newsapi.org. Every API call is going to be unique. For newapi, there are two major endpoints : Everything and Top Headlines. We will use Top Headlines and specify the following details in the URL : 

- country : us 
- category : business, health, entertainment 
- from : 2022-09-01
- language : en
- API key : which is generated uniquely for each user

After we GET the URL, we convert the raw content of the URL to character using `rawToChar()` function. We notice that this is in JSON format. Reading a JSON file is quite a simple task. One can extract and read the data of a JSON file very efficiently using the `fromJSON()` function. Once we have the JSON data read in, we convert the first level of JSON iterable to string and extract source, author, and title columns from the articles dataframe that stores the actual information.

```{r}
business <- GET("https://newsapi.org/v2/top-headlines/?country=us&category=business&from=2022-09-01&language=en&apiKey=0bf60d64a65d49a88c64d4f4e8565b1d")
business_parsed <- fromJSON(rawToChar(business$content))
str(business_parsed, max.level = 1)
business_parsed$articles %>% 
  select(source, author, title)

health <- GET("https://newsapi.org/v2/top-headlines/?country=us&category=health&from=2022-09-01&language=en&apiKey=0bf60d64a65d49a88c64d4f4e8565b1d")
health_parsed <- fromJSON(rawToChar(health$content))
str(health_parsed, max.level = 1)
health_parsed$articles %>% 
  select(source, author, title)


technology <- GET("https://newsapi.org/v2/top-headlines/?country=us&category=technology&from=2022-09-01&language=en&apiKey=0bf60d64a65d49a88c64d4f4e8565b1d")
technology_parsed <- fromJSON(rawToChar(technology$content))
str(technology_parsed, max.level = 1)
technology_parsed$articles %>% 
  select(source, author, title)

```





